<h1>Analizando tweets</h1>
<p>Fecha l√≠mite: domingo 14 de marzo a las 11:59 pm.</p>
<p>El prop√≥sito de esta tarea es darte la oportunidad de practicar</p>
<ol>
<li>El uso de diccionarios para representar datos y como un mecanismo para asignar claves a valores que cambian con el tiempo,</li>
<li>Usar funciones para evitar c√≥digo repetido,</li>
<li>Estructurar una tarea en piezas l√≥gicas y f√°ciles de probar, y</li>
<li>Leer la documentaci√≥n para las funciones de la biblioteca.</li>
</ol>
<h2>Introducci√≥n</h2>
<p>Para esta tarea, analizaremos los tweets que se han hecho en las √∫ltimas semanas respecto al NAICM y la controversia por la Auditor√≠a de la ASF.</p>
<h3>Empezando</h3>
<p>Antes de comenzar a trabajar en las tareas de la tarea, asegurate de haber descargado el archivo twitter_NAICM_full.json, el archivo util.py que contienen los datos y funciones de ayuda para correr el c√≥digo, y el archivo analysis.py que contiene las funciones a modificar. Deber√°s entregar el archivo de analysis.py con las funciones, aunque puedes usar un notebook para hacer pruebas si as√≠ lo consideras pertinente.
Debes guardar todos los archivos en una sola carpeta para asegurarte que el c√≥digo funciona como esperarar√≠as. En esta tarea, puede asumir que la entrada pasada a sus funciones tiene el formato correcto. No puede alterar ninguna de los inputs que se pasan a las funciones de util.py y analysis.py. En general, es de mal estilo modificar una estructura de datos pasada como entrada a una funci√≥n, a menos que ese sea el prop√≥sito expl√≠cito de la funci√≥n. Si alguien llama a su funci√≥n, es posible que tenga otros usos para los datos y no deber√≠a sorprenderse con cambios inesperados.</p>
<h3>Parte 1: Algoritmos b√°sicos</h3>
<p>Los algoritmos para contar y clasificar de manera eficiente tokens distintos , o valores √∫nicos, se utilizan ampliamente en el an√°lisis de datos. Por ejemplo, es posible que deseemos encontrar la palabra clave m√°s utilizada en un documento o los 10 enlaces m√°s populares en los que se hace clic en un sitio web. En la Parte 1, implementar√° dos de estos algoritmos: <code>find_top_k</code> y <code>find_min_count</code>. Tambi√©n implementar√° un algoritmo para encontrar los tokens m√°s destacados (es decir, los m√°s importantes o notables) de un documento en el contexto de una colecci√≥n de documentos.</p>
<p>En esta parte, agregar√° c√≥digo a analysis.py para implementar los algoritmos descritos en las siguientes subsecciones.</p>
<h4>Paso 1.1: Contar tokens distintos</h4>
<p>El primer paso es escribir una funci√≥n auxiliar count_tokens, que cuente distintos tokens. Esta funci√≥n toma como entrada una lista de tokens (en nuestro ejemplo son string, pero pueden ser de cualquier tipo inmutable) y devuelve un diccionario que asigna tokens a recuentos.</p>
<p>Por ejemplo, si tenemos una lista:</p>
<p><code>['A', 'B', 'C', 'A']</code></p>
<p>la funci√≥n deber√≠a producir (el orden exacto de los pares clave-valor en el diccionario no es importante):</p>
<p><code>{'A': 2, 'B': 1, 'C': 1}</code></p>
<p>Notas:</p>
<ol>
<li>
<p>No utilice bibliotecas de Python distintas de las que ya se han importado. (Por ejemplo, no utilice collections.Counter.)</p>
</li>
<li>
<p>Si usa el list.count() m√©todo de Python, su soluci√≥n ser√° ineficiente y no recibir√° cr√©dito por esta tarea.</p>
</li>
</ol>
<h4>Paso 1.2: Top K</h4>
<p>Escribir√° el algoritmo find_top_k, que toma una lista de tokens y un entero k no negativo , y devuelve una lista de los tokens k que ocurren con mayor frecuencia en la lista. El resultado debe clasificarse en orden descendente de frecuencia de token.</p>
<p>A continuaci√≥n, se muestra un ejemplo de uso de esta funci√≥n:</p>
<pre><code>In [4]: l = ['D', 'B', 'C', 'D', 'D', 'B', 'D', 'C', 'D', 'A']

In [5]: find_top_k(l, 2)
Out[5]: ['D','B
</code></pre>
<p>Para hacer este c√°lculo, necesitar√°:</p>
<ol>
<li>
<p>Contar el n√∫mero de veces que aparece cada ficha,</p>
</li>
<li>
<p>Convertir esos datos en una lista de (token, recuento) tuplas</p>
</li>
<li>
<p>Ordenar la lista resultante en orden decreciente por recuento.</p>
</li>
<li>
<p>Extraer los tokens K que ocurrieron con m√°s frecuencia.</p>
</li>
</ol>
<p>La funci√≥n sort de Python ordena los pares utilizando el primer valor como clave principal y el segundo valor como clave secundaria (para romper los lazos) y, por lo tanto, no es adecuada para esta tarea. En su lugar, necesita una funci√≥n de sorting que utilice el segundo valor como llave principal y el primer valor para romper los empates. Hemos proporcionado tal funci√≥n (puede usar la tuya):</p>
<pre><code>sorted(list(ngram_dict.items()), key=lambda x: x[1], reverse=True)
</code></pre>
<h4>Paso 1.3: N√∫mero m√≠nimo de ocurrencias</h4>
<p>Escribir√°s el algoritmo find_min_count, que calcula un conjunto de tokens en una lista que ocurren al menos un n√∫mero m√≠nimo de veces especificado.</p>
<p>A continuaci√≥n, se muestra un ejemplo de uso de esta funci√≥n:</p>
<pre><code>In [6]: l = ['D', 'B', 'C', 'D', 'D', 'B', 'D', 'C', 'D', 'A']

In [7]: basic_algorithms.find_min_count(l, 2)
Out[7]: {'D', 'B', 'C'}
</code></pre>
<p>Recuerde que los conjuntos no est√°n ordenados, por lo que el resultado de esta llamada podr√≠a haber sido f√°cilmente .</p>
<p><code>{'B', 'C', 'D'}</code></p>
<h3>Tokens salientes</h3>
<p>Es posible que los tokens m√°s frecuentes de la lista no sean los m√°s destacados. Por ejemplo, si la lista contiene palabras de un documento en ingl√©s, el hecho de que las palabras ‚Äúa‚Äù, ‚Äúan‚Äù y ‚Äúthe‚Äù aparezcan con frecuencia no es sorprendente.</p>
<p>En procesamiento de textos, el t√©rmino frecuencia-frecuencia inversa de documentos (tambi√©n conocido como tf-idf ) es una estad√≠stica dise√±ada para reflejar la importancia de una palabra para un documento en una colecci√≥n o corpus y se utiliza a menudo como factor de ponderaci√≥n en la recuperaci√≥n de informaci√≥n y miner√≠a de texto. Una palabra o t√©rmino se considera relevante para un documento en particular si aparece con frecuencia en ese documento, pero no en el corpus del documento en general.</p>
<p>Definiciones</p>
<p>T√©rmino frecuencia - frecuencia inversa de documentos se define como:</p>
<p><img src="https://latex.codecogs.com/gif.latex?tf_idf(t,&amp;space;d,&amp;space;D)&amp;space;=&amp;space;tf(t,&amp;space;d)&amp;space;\cdot&amp;space;idf(t,&amp;space;D)" title="tf_idf(t, d, D) = tf(t, d) \cdot idf(t, D)"></p>
<p>d√≥nde t es un t√©rmino, d es un documento (colecci√≥n de t√©rminos), D es una colecci√≥n de documentos, td y idf se definen a continuaci√≥n.</p>
<p>Hay varias variantes de frecuencia de ambos t√©rminos (tf) y frecuencia inversa del documento (idf) que se puede utilizar para calcular. Usaremos la frecuencia aumentada como nuestra medida de la frecuencia de los t√©rminos para evitar el sesgo hacia documentos m√°s largos, y usaremos la frecuencia inversa b√°sica de los documentos.</p>
<p>La frecuencia aumentada de un t√©rmino en un documento. Se define como</p>
<p><img src="https://latex.codecogs.com/gif.latex?tf(t,&amp;space;d)&amp;space;=&amp;space;0.5&amp;space;+&amp;space;0.5&amp;space;\cdot&amp;space;\left&amp;space;(&amp;space;\frac{f_{t,d}}{\max(\{f_{t^\prime,d}:&amp;space;t^\prime&amp;space;\in&amp;space;d\})}&amp;space;\right&amp;space;)" title="tf(t, d) = 0.5 + 0.5 \cdot \left ( \frac{f_{t,d}}{\max(\{f_{t^\prime,d}: t^\prime \in d\})} \right )"></p>
<p>d√≥nde <img src="https://latex.codecogs.com/gif.latex?f_{t,d}" title="f_{t,d}"> es el n√∫mero de veces que el t√©rmino  aparece en el documento .</p>
<p>La frecuencia b√°sica de documento inverso de un t√©rmino en una colecci√≥n de documentos  Se define como</p>
<p><img src="https://latex.codecogs.com/gif.latex?idf(t,&amp;space;D)&amp;space;=&amp;space;\log&amp;space;\left&amp;space;(&amp;space;\frac{N}{\lvert&amp;space;\{d&amp;space;\in&amp;space;D&amp;space;:&amp;space;t&amp;space;\in&amp;space;d\}&amp;space;\rvert}&amp;space;\right&amp;space;)" title="idf(t, D) = \log \left ( \frac{N}{\lvert \{d \in D : t \in d\} \rvert} \right )"></p>
<p>d√≥nde N es el n√∫mero de documentos en la colecci√≥n de documentos D y donde <img src="https://latex.codecogs.com/gif.latex?{\lvert&amp;space;\{d&amp;space;\in&amp;space;D&amp;space;:&amp;space;t&amp;space;\in&amp;space;d\}&amp;space;\rvert}" title="{\lvert \{d \in D : t \in d\} \rvert}">  es el n√∫mero de documentos en los que ocurre el t√©rmino t. Utilice el log natural ( math.log) en su c√°lculo de idf.</p>
<p>Dado un umbral , el conjunto de palabras destacadas de un documento  en una colecci√≥n de documentos  Se define como:</p>
<p><img src="https://latex.codecogs.com/gif.latex?salient(d,&amp;space;D,&amp;space;T)&amp;space;=&amp;space;\{&amp;space;t&amp;space;:&amp;space;t&amp;space;\in&amp;space;d\,&amp;space;\rm{and}&amp;space;,&amp;space;tfidf(t,&amp;space;d,&amp;space;D)&amp;space;>&amp;space;T&amp;space;\}" title="salient(d, D, T) = \{ t : t \in d\, \rm{and} , tfidf(t, d, D) > T \}"></p>
<p>Ejemplo</p>
<p>En esta secci√≥n, trabajaremos en el c√°lculo de los tokens destacados para cada documento en una colecci√≥n de documentos de muestra. Aqu√≠ est√° la colecci√≥n de muestra:</p>
<pre><code>  [['D', 'B', 'D', 'C', 'D', 'C', 'C'],
   ['D', 'A', 'A'],
   ['D', 'B'],
   []]
</code></pre>
<p>Estos son los valores de frecuencia de t√©rminos aumentados para cada documento:</p>
<pre><code>  Document: ['D', 'B', 'D', 'C', 'D', 'C', 'C']
  Token: TF
    B:  0.6666666666666666
    C:  1.0
    D:  1.0

  Document: ['D', 'A', 'A']
  Token: TF
    A:  1.0
    D:  0.75

  Document: ['D', 'B']
  Token: TF
    B:  1.0
    D:  1.0

  Document: []
</code></pre>
<p>No hay valores de frecuencia de t√©rminos aumentados para el √∫ltimo documento porque est√° vac√≠o.</p>
<p>Estos son los valores de frecuencia de documentos inversos para la colecci√≥n de documentos:</p>
<pre><code>  Token: IDF:
    A: 1.3862943611198906
    B: 0.6931471805599453
    C: 1.3862943611198906
    D: 0.28768207245178085
</code></pre>
<p>Aqu√≠ est√° el c√°lculo de prominencia para cada documento usando un umbral de 0.4.</p>
<pre><code>  Document: ['D', 'B', 'D', 'C', 'D', 'C', 'C']
  Token: The token is salient (TF_IDF)
    B: True (0.46209812037329684)
    C: True (1.3862943611198906)
    D: False (0.28768207245178085)
  Salient tokens: {'B', 'C'}

  Document: ['D', 'A', 'A']
  Token: The token is salient (TF_IDF)
    A: True (1.3862943611198906)
    D: False (0.21576155433883565)
  Salient tokens: {'A'}

  Document: ['D', 'B']
  Token: The token is salient (TF_IDF)
    B: True (0.6931471805599453)
    D: False (0.28768207245178085)
  Salient tokens: {'B'}

  Document: []
  Salient tokens: set()
</code></pre>
<p>Tenga en cuenta que el documento vac√≠o no tiene tokens destacados.</p>
<p>Finalmente, el resultado de calcular los tokens destacados para los documentos en la colecci√≥n de documentos es:</p>
<pre><code>    [{'B', 'C'}, {'A'}, {'B'}, set()]
</code></pre>
<h4>Paso 1.4: Calcular tokens destacados</h4>
<p>En esta tarea, escribir√° el algoritmo find_salient, que toma una colecci√≥n de documentos ( docs) y un umbral de punto flotante ( threshold) y encuentra el conjunto de palabras destacadas para cada documento (como se define en la secci√≥n anterior). El resultado de la funci√≥n ser√° una lista de conjuntos, con un conjunto por documento.</p>
<p>Debe pensar detenidamente c√≥mo organizar el c√≥digo para implementar este algoritmo. No pongan todo el c√≥digo para esta tarea en una sola funci√≥n.</p>
<p>Algunas cosas para tener en cuenta:</p>
<ol>
<li>
<p>Necesitar√° una estructura de datos intermedia para realizar un seguimiento de las frecuencias de documentos inversas para las palabras en la colecci√≥n de documentos.</p>
</li>
<li>
<p>Puede resultarle √∫til calcular una estructura de datos con el t√©rmino frecuencias para las palabras de un documento, pero no es estrictamente necesario.</p>
</li>
<li>
<p>Recuerde que el conjunto de palabras destacadas de un documento vac√≠o es solo el conjunto vac√≠o.</p>
</li>
<li>
<p>Probar sus funciones intermedias (en notebook) a medida que avanza har√° que sea m√°s f√°cil completar esta tarea con √©xito.</p>
</li>
</ol>
<h3>Analizando Tweets</h3>
<p>Ahora que ha implementado los algoritmos b√°sicos, puede comenzar a analizar los feeds de Twitter. Para el resto de las tareas, modifique el c√≥digo analysis.py.</p>
<p>Si bien proporcionamos encabezados de funci√≥n para cada una de las tareas requeridas, la estructura del resto del c√≥digo depende de usted. Algunas tareas se pueden realizar limpiamente con una funci√≥n, otras no. Esperamos que busque subtareas comunes para abstraerlas en funciones y esperamos que reutilice las funciones completadas anteriormente. Este proceso de descomposici√≥n de funciones y reutilizaci√≥n cuidadosa de funciones es una de las claves para escribir c√≥digo limpio. Puede colocar sus funciones auxiliares en cualquier lugar del archivo que tenga sentido para usted.</p>
<h4>Datos</h4>
<p>Twitter nos permite buscar tweets con propiedades particulares, por ejemplo, de un usuario en particular, que contienen t√©rminos espec√≠ficos y dentro de un rango de fechas determinado. Hay varias bibliotecas de Python que simplifican el proceso de uso de esta funci√≥n de Twitter. Se utiliz√≥ la biblioteca Tweepy para recoger tweets sobre el NAICM, y almacenar los datos resultantes en archivos JSON.</p>
<h4>Representando tweets</h4>
<p>Un solo tweet est√° representado por un diccionario que contiene mucha informaci√≥n: tiempo de creaci√≥n, hashtags utilizados, usuarios mencionados, texto del tweet, etc. Por ejemplo, aqu√≠ hay un tweet enviado por @UKLabour:</p>
<pre><code>RT @MXvsCORRUPCION: ¬øQu√© sabemos del costo de cancelar el Nuevo Aeropuerto en Texcoco? ¬øQu√© ganamos los mexicanos con esta decisi√≥n? Aqu√≠ @‚Ä¶
</code></pre>
<p>y aqu√≠ hay una versi√≥n abreviada del correspondiente diccionario de tweets que incluye algunos de los m√°s de 20 pares clave / valor:</p>
<pre><code>{'contributors': None,
 'coordinates': None,
 'created_at': 'Thu Feb 25 23:18:11 +0000 2021',
 'entities': {'hashtags': [],
  'symbols': [],
  'urls': [],
  'user_mentions': [{'id': 707633643150135296,
    'id_str': '707633643150135296',
    'indices': [3, 18],
    'name': 'MXvsCORRUPCI√ìN',
    'screen_name': 'MXvsCORRUPCION'}]},
 'favorite_count': 0,
 'favorited': False,
 'geo': None,
 'id': 1365078669014822915,
 'retweet_count': 64,
 'retweeted': False,
 'retweeted_status': {'contributors': None,
  'coordinates': None,
  'created_at': 'Thu Feb 25 23:15:14 +0000 2021',
  'entities': {'hashtags': [],
   'symbols': [],
   'urls': [{'display_url': 'twitter.com/i/web/status/1‚Ä¶',
     'expanded_url': 'https://twitter.com/i/web/status/1365077926190972937',
     'indices': [117, 140],
     'url': 'https://t.co/PlWXrOHQp9'}],
   'user_mentions': []},
  'favorite_count': 125,
  'favorited': False,
  'geo': None,
  'source': '&lt;a href="https://mobile.twitter.com" rel="nofollow"&gt;Twitter Web App&lt;/a&gt;',
  'text': '¬øQu√© sabemos del costo de cancelar el Nuevo Aeropuerto en Texcoco? ¬øQu√© ganamos los mexicanos con esta decisi√≥n? Aq‚Ä¶ https://t.co/PlWXrOHQp9',
  'truncated': True,
  'user': {'contributors_enabled': False,
   'created_at': 'Wed Mar 09 18:26:22 +0000 2016',
   'default_profile': False,
   'default_profile_image': False,
   'description': 'Analizamos la corrupci√≥n, hacemos periodismo, buscamos justicia, movilizamos socialmente y conversamos.',
   'entities': {'description': {'urls': []},
 'text': 'RT @MXvsCORRUPCION: ¬øQu√© sabemos del costo de cancelar el Nuevo Aeropuerto en Texcoco? ¬øQu√© ganamos los mexicanos con esta decisi√≥n? Aqu√≠ @‚Ä¶',
 'truncated': False}
</code></pre>
<p>En conjunto, los hashtags, los s√≠mbolos, las menciones de usuarios y las URL se denominan entidades . Se puede acceder a estas entidades a trav√©s de la "entities"clave en el diccionario del tweet. El valor asociado a la clave "entities"es en s√≠ mismo un diccionario que mapea claves que representan los tipos de entidad ( "hashtags", "symbols", "urls", "user_mentions") las listas de entidades de ese tipo. En general, la estructura de datos de las entidades tiene la forma:</p>
<pre><code>'entities': {'key1': [{'subkey1': value11, 'subkey2': value21},
                      {'subkey1': value12, 'subkey2': value22},
                      ...],
             'key2': [{'subkey3': value31, 'subkey4': value41},
                      {'subkey3': value32, 'subkey4': value42},
                     ...],
             ...}
</code></pre>
<p>Por ejemplo, aqu√≠ hay un subconjunto de las entidades del tweet de muestra:</p>
<pre><code>{'hashtags': [{'indices': [94, 100], 'text': 'NAICM'}],
 'symbols': [],
 'urls': [],
 'user_mentions': [{'id': 114900050,
   'id_str': '114900050',
   'indices': [3, 16],
   'name': 'Mario Riestra Pi√±a',
   'screen_name': 'marioriestra'}]}
</code></pre>
<p>Cada entidad individual se representa con un diccionario, cuya forma depende del tipo de entidad. Por ejemplo, un hashtag incluir√° informaci√≥n sobre el hashtag e indices d√≥nde ocurri√≥ en el tweet:</p>
<pre><code>{'indices': [62, 74], 'text': 'GetInvolved'}
</code></pre>
<h3>Explorando los datos</h3>
<p>Para simplificar el proceso de exploraci√≥n de datos y pruebas, hemos proporcionado un c√≥digo para cargar los tweets por usted. Aqu√≠ lo tienes:</p>
<pre><code>import json

with open("twitter_NAICM_full.json", "r+") as jsonFile:
    NAICM = json.load(jsonFile)
    
# sample tweet 
tweet0 = NAICM[0]
</code></pre>
<p>Le recomendamos que juegue con un par de diccionarios de tweets para familiarizarse con c√≥mo acceder a la informaci√≥n en la estructura de datos antes de pasar a la siguiente tarea.</p>
<h3>Parte 2: Encontrar entidades que ocurren com√∫nmente</h3>
<h5>Par√°metros comunes para la Parte 2</h5>
<p>Las siguientes dos actividades utilizar√°n dos de los mismos par√°metros. Para evitar repeticiones posteriores, las describimos aqu√≠:</p>
<ul>
<li>
<p>tweets es una lista de diccionarios que representan tweets.</p>
</li>
<li>
<p>entity_desc es una tupla con tres valores: el primero es el tipo de entidad de inter√©s (la clave), el segundo es la informaci√≥n de inter√©s para ese tipo de entidad (la subclave) y el tercero es un booleano que indica si los valores diferencia may√∫sculas y min√∫sculas . Por ejemplo, usamos la tupla para describir los hashtags, porque es la clave para extraer los hashtags del diccionario de entidades, es la subclave para extraer el texto del hashtag del diccionario de entidades y, para nuestros prop√≥sitos, usualmente trataremos hashtags que no distinguen entre may√∫sculas y min√∫sculas. Por ejemplo, al trabajar con hashtags trataremos de ser iguales a . Por otro lado, las URL distinguen entre may√∫sculas y min√∫sculas, por lo que las usar√≠amos como descripci√≥n de entidad para las URL.("hashtags", "text", False)"hashtags""text""#bbcqt""#BBCQT"("urls", "url", True)</p>
</li>
</ul>
<p>(Encontrar√° que el m√©todo lower de string es √∫til para manejar entidades que no distinguen entre may√∫sculas y min√∫sculas).</p>
<h4>Paso 2.1: Principales entidades K</h4>
<p>Para el paso 2.1, escribir√°s una funci√≥n que encuentre las k entidades m√°s comunes en una lista de tweets usando el algoritmo que escribiste en analysis.py. Debes completar la siguiente funci√≥n:</p>
<pre><code>def find_top_k_entities(tweets, entity_desc, k):
</code></pre>
<p>Los dos primeros par√°metros son los descritos anteriormente y k es un n√∫mero entero. Esta funci√≥n, que est√° en analysis.py, deber√≠a devolver una lista de las kentidades m√°s comunes. Como en la Tarea 1.2, la entidad m√°s com√∫n debe ser la primera, la siguiente entidad m√°s com√∫n debe ser la segunda, etc.</p>
<p>Aqu√≠ hay una llamada de muestra:</p>
<pre><code>In [13]: find_top_k_entities(NAICM[:50], ("hashtags", "text"), 3)
Out[13]: ['asf', 'regeneraci√≥nmx', 'naicm']
</code></pre>
<h4>Paso 2.2: N√∫mero m√≠nimo de ocurrencias</h4>
<p>Para la Tarea 2.2, encontrar√° todas las entidades que aparecen un n√∫mero m√≠nimo de veces utilizando el min_count que escribi√≥ anteriormente. Debes completar la funci√≥n:</p>
<pre><code>def find_min_count_entities(tweets, entity_desc, min_count):
</code></pre>
<p>donde los dos primeros par√°metros son los descritos anteriormente y min_count es un n√∫mero entero que especifica el n√∫mero m√≠nimo de veces que debe ocurrir una entidad para ser incluida en el resultado. Esta funci√≥n deber√≠a devolver un set de entidades ocurridas al menos min_count veces.</p>
<p>Aqu√≠ hay un ejemplo de uso de esta funci√≥n usando los primeros 500 tweets:</p>
<pre><code>In [14]:find_min_count_entities(NAICM[:500], ("user_mentions", "name"), 10)
Out[14]: {'arturo herrera guti√©rrez',
 'francisco jimenez #vxm',
 'adriana h',
 'pablo pi√±a üá≤üáΩ cdmx',
 'eric blair',
 'gabs ¬Æ... ‚Ñõ‚ÑØùìÉùí∂ùí∏ùíæùíπùí∂ ‚ôî',
 'carlos loret de mola',
 'esmeralda river√≥n',
 'rochamonero',
 'latinus'}
</code></pre>
<h3>Parte 3: Analizando N-gramos</h3>
<p>¬øQu√© conocimientos adicionales podr√≠amos obtener al analizar palabras y secuencias de palabras?</p>
<p>En esta parte, aplicar√° los tres algoritmos descritos anteriormente a secuencias contiguas de palabras, que se conocen como n-gramas . Antes de aplicar estos algoritmos a los tweets de un candidato, procesar√° previamente los tweets para revelar palabras √∫tiles y luego extraer√° los n-gramas. Su soluci√≥n debe usar funciones auxiliares para evitar c√≥digo duplicado.</p>
<p>Hemos proporcionado tres funciones que ayudan a procesar y limpiar texto</p>
<pre><code>import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stopwords = stopwords.words("spanish")
stopwords += ["rt", "https", "t", "co"]

with open("twitter_NAICM_full.json", "r+") as jsonFile:
    NAICM = json.load(jsonFile)

tweet_clean = remove_hashtags(NAICM[0]["text"])
tweet_text = eliminar_nooalfanum(tweet_clean)
tweet_final = quitar_palabrasrep(tweet_text , stopwords)
</code></pre>
<p>Este c√≥digo te arrojar√° una lista</p>
<pre><code>['sabemos',
 'costo',
 'cancelar',
 'nuevo',
 'aeropuerto',
 'texcoco',
 'ganamos',
 'mexicanos',
 'decisi√≥n',
 'aqu√≠',
 '']
</code></pre>
<h4>Representar N-gramos</h4>
<p>Su c√≥digo debe calcular los n-gramas de un tweet despu√©s de preprocesar el texto abreviado del tweet. Estos n-gramas deben representarse como tuplas de cadenas. Considere la siguiente lista (que proviene del procesamiento previo):</p>
<pre><code>['sabemos',
 'costo',
 'cancelar',
 'nuevo',
 'aeropuerto',
 'texcoco',
 'ganamos',
 'mexicanos',
 'decisi√≥n',
 'aqu√≠',
 '']
</code></pre>
<p>Tomando  producir√≠a los siguientes bi-gramos (2 gramos):</p>
<pre><code>[('sabemos', 'costo'),
 ('costo', 'cancelar'),
 ('cancelar', 'nuevo'),
 ('nuevo', 'aeropuerto'),
 ('aeropuerto', 'texcoco'),
 ('texcoco', 'ganamos'),
 ('ganamos', 'mexicanos'),
 ('mexicanos', 'decisi√≥n'),
 ('decisi√≥n', 'aqu√≠'),
 ('aqu√≠', '')]
</code></pre>
<p>Notas</p>
<p>El n-grama no "regresa" al principio. Es decir, la √∫ltima palabra del tweet y la primera palabra del tweet no comprenden un n-gram(por lo que no se incluye).('now', 'things')</p>
<p>Usted debe no combinar palabras de diferentes tuits en un sola n-gram.</p>
<p>Par√°metros comunes para la Parte 3
El resto de pasos tienen tres par√°metros en com√∫n:</p>
<p>tweets una lista de diccionarios que representan tweets</p>
<p>n es el n√∫mero de palabras en un n-grama.</p>
<p>case_sensitive un booleano, es decir, True si la tarea distingue entre may√∫sculas y min√∫sculas.</p>
<h4>Paso 3.1: Top K n-gramas</h4>
<p>Aplicar√° su find_top_kfunci√≥n escrita anteriormente para encontrar los n-gramas m√°s comunes. Tu tarea es implementar la funci√≥n:</p>
<p>def find_top_k_ngrams(tweets, n, case_sensitive, k):</p>
<p>donde los primeros tres par√°metros son los descritos anteriormente y k es un n√∫mero entero. Esta funci√≥n deber√≠a devolver una lista de los -gramas km√°s comunes n. Como en la Tarea 1.2, el n-grama m√°s com√∫n debe ir primero, seguido del segundo m√°s com√∫n, etc.</p>
<p>He aqu√≠ un ejemplo de uso de esta:</p>
<pre><code>In [16]: find_top_k_ngrams(NAICM[:10000], 3, 10)
Out[16]: ['75 ciento reportado',
 'menos 75 ciento',
 'primer an√°lisis menos',
 'an√°lisis menos 75',
 'revisamos reporte asf',
 'asf primer an√°lisis',
 'reporte asf primer',
 'ciento reportado respecto',
 'reportado respecto can',
 'respecto can ']
</code></pre>
<h3>Paso 3.2: N-gramas destacados</h3>
<p>Finalmente, usar√° su find_salient de la Parte 1 para encontrar los n-gramas destacados en cada tweet de una colecci√≥n. Tu tarea es implementar la funci√≥n:</p>
<p>def find_salient_ngrams(tweets, n, case_sensitive, threshold):</p>
<p>donde los primeros tres par√°metros son los descritos anteriormente y thresholdes elumbral para decidir que un n-gramo es sobresaliente. Esta funci√≥n deber√≠a devolver una lista de conjuntos de n-gramas destacados, un conjunto por tweet.</p>
<p>Aqu√≠ hay un ejemplo de uso de esta funci√≥n:</p>
<pre><code>In [68]: tweets = [ {"abridged_text": "the cat in the hat" },
    ...:            {"abridged_text": "don't let the cat on the hat" },
    ...:            {"abridged_text": "the cat's hat" },
    ...:            {"abridged_text": "the hat cat" }]
    ...:
In [69]: find_salient_ngrams(tweets, 2, False, 1.3)
Out[69]:
[{('cat', 'in'), ('in', 'the')},
 {('cat', 'on'), ("don't", 'let'), ('let', 'the'), ('on', 'the')},
 {("cat's", 'hat'), ('the', "cat's")},
 {('hat', 'cat')}]
</code></pre>
<h3>Finalizando</h3>
<p>Para acabar agrega en la funci√≥n de analysis las siguientes llamadas a funci√≥n:</p>
<p>Los 10 principales hashtags por d√≠a (la key es created_at, vas a tener que limpiar para obtener el d√≠a)</p>
<p>Los principales usuarios mencionados por d√≠a (que tuvieron al menos 50, 100, 250 y 500 menciones)</p>
<p>Los principales usuarios mencionados por d√≠a (que tuvieron al menos 50, 100, 250 y 500 menciones) de los tweets que fueron favorited (la llave es "favorited")</p>
<p>Los 10 principales 2-grams y 3-grams por d√≠a</p>
<p>Los 10 principales 2-grams y 3-grams de los tweets que fueron favorited</p>
<p>Los 10 principales 2-grams y 3-grams que fueron retweeteados m√°s de 25 veces
(retweeted tiene que ser True y retweet_count mayor a 30</p>
<p>Los 2-grams y 3-grams salientes con un threshold arriba de .5, 1, 1.5 y 2.5</p>
